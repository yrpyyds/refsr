import os
import subprocess
import multiprocessing as mp
import time
import logging

# ========== æ—¥å¿—é…ç½® ==========
log_dir = "/root/autodl-tmp/sr/logs"
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"batch_train_{time.strftime('%Y%m%d_%H%M%S')}.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    handlers=[
        logging.FileHandler(log_file, encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ========== åŸºç¡€é…ç½® ==========
base_train_script = "/root/autodl-tmp/sr/SeeSR/train_dreambooth_lora_textual_inversion.py"
base_dataset_root = "/root/autodl-tmp/sr/datasets/train/dataset/"
output_dir_root = "/root/autodl-tmp/sr/models/output_1109_batch/dreambooth"
os.makedirs(output_dir_root, exist_ok=True)

# ç±»åˆ«åˆ—è¡¨
categories = sorted(os.listdir(base_dataset_root))
gpus = ["0"]  # ä½ åªæœ‰ä¸€å¼  GPUï¼Œæ‰€ä»¥å¤šè¿›ç¨‹å…±ç”¨åŒä¸€ GPU
group_size = 3  # âœ… æ¯æ¬¡å¹¶è¡Œ 3 ä¸ªç±»åˆ«

# ========== è¾…åŠ©å‡½æ•° ==========
def run_training(cls_name, gpu_id):
    """è¿è¡Œå•ä¸ªç±»åˆ«è®­ç»ƒ"""
    # æ¯ä¸ªç±»åˆ«å•ç‹¬æ—¥å¿—
    cls_log = os.path.join(log_dir, f"train_{cls_name}_{time.strftime('%Y%m%d_%H%M%S')}.log")
    cls_logger = logging.getLogger(cls_name)
    cls_handler = logging.FileHandler(cls_log, encoding="utf-8")
    cls_handler.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))
    cls_logger.addHandler(cls_handler)
    cls_logger.setLevel(logging.INFO)

    cls_logger.info(f"ğŸŸ¢ å¯åŠ¨ç±»åˆ«ï¼š{cls_name} (GPU {gpu_id})")

    instance_data_dir = os.path.join(base_dataset_root, cls_name, "train")
    ti_initializer_token = cls_name
    instance_prompt = f"a photo of sks {cls_name}"
    validation_prompt = f"a boy with a sks {cls_name}"

    pattern = "lora-dreambooth-ti-model"
    model_name = f"{pattern}-{cls_name}"
    output_dir = os.path.join(output_dir_root, model_name)

    cmd = [
        "python", base_train_script,
        "--ti_initializer_token", ti_initializer_token,
        "--instance_data_dir", instance_data_dir,
        "--instance_prompt", instance_prompt,
        "--validation_prompt", validation_prompt,
        "--output_dir", output_dir,
    ]

    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = gpu_id

    cls_logger.info(f"ğŸš€ GPU {gpu_id} è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
    start_time = time.time()
    subprocess.run(cmd, env=env)
    end_time = time.time()

    duration = (end_time - start_time) / 60
    cls_logger.info(f"âœ… ç±»åˆ« {cls_name} è®­ç»ƒå®Œæˆ (GPU {gpu_id})ï¼Œè€—æ—¶ {duration:.2f} åˆ†é’Ÿ")
    cls_logger.removeHandler(cls_handler)
    cls_handler.close()


# ========== ä¸»æµç¨‹ ==========
def main():
    logger.info("============== å¯åŠ¨æ‰¹é‡è®­ç»ƒï¼ˆæ¯æ¬¡å¹¶è¡Œ 3 ç±»ï¼‰ ==============")
    logger.info(f"æ€»ç±»åˆ«æ•°: {len(categories)} | æ—¥å¿—æ–‡ä»¶: {log_file}\n")

    for i in range(0, len(categories), group_size):
        group = categories[i:i + group_size]
        logger.info(f"ğŸš€ å¯åŠ¨æ‰¹æ¬¡ï¼š{group}")

        processes = []
        start_batch_time = time.time()

        for cls in group:
            p = mp.Process(target=run_training, args=(cls, gpus[0]))
            p.start()
            processes.append(p)

        # ç­‰è¿™ä¸€æ‰¹çš„ 3 ä¸ªè¿›ç¨‹éƒ½è®­ç»ƒå®Œ
        for p in processes:
            p.join()

        end_batch_time = time.time()
        batch_dur = (end_batch_time - start_batch_time) / 60
        logger.info(f"âœ… æ‰¹æ¬¡ {group} å®Œæˆï¼Œè€—æ—¶ {batch_dur:.2f} åˆ†é’Ÿï¼Œç­‰å¾… 10 ç§’è¿›å…¥ä¸‹ä¸€ä¸ªæ‰¹æ¬¡...\n")
        time.sleep(10)

    logger.info("ğŸ‰ å…¨éƒ¨ç±»åˆ«è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
